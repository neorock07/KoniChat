{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9119136,"sourceType":"datasetVersion","datasetId":5504715},{"sourceId":9119269,"sourceType":"datasetVersion","datasetId":5504807},{"sourceId":9144340,"sourceType":"datasetVersion","datasetId":5523085},{"sourceId":9154748,"sourceType":"datasetVersion","datasetId":5530295},{"sourceId":9154847,"sourceType":"datasetVersion","datasetId":5530345},{"sourceId":9158583,"sourceType":"datasetVersion","datasetId":5533006}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output  \n!pip install -U langchain-community\n!pip install langchain\n!pip install -U langchain-huggingface\nclear_output()","metadata":{"_uuid":"912b37c5-7d8d-4f75-ac69-3939d54ad83f","_cell_guid":"817493b1-92e9-47a5-b20e-7e0c91e5b32e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:11:35.554614Z","iopub.execute_input":"2024-08-14T06:11:35.555582Z","iopub.status.idle":"2024-08-14T06:12:13.912022Z","shell.execute_reply.started":"2024-08-14T06:11:35.555543Z","shell.execute_reply":"2024-08-14T06:12:13.910959Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install -y pciutils\n!curl https://ollama.ai/install.sh | sh","metadata":{"_uuid":"c90fcd16-4dc0-4316-a2cf-3faca9b6dac6","_cell_guid":"82ca2ab6-c961-48f4-8a92-abd717a04b97","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:12:13.914398Z","iopub.execute_input":"2024-08-14T06:12:13.915187Z","iopub.status.idle":"2024-08-14T06:12:23.009557Z","shell.execute_reply.started":"2024-08-14T06:12:13.915140Z","shell.execute_reply":"2024-08-14T06:12:23.008693Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\npciutils is already the newest version (1:3.6.4-1ubuntu0.20.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 80 not upgraded.\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n100 12030    0 12030    0     0  51008      0 --:--:-- --:--:-- --:--:-- 51191\n######################################################################## 100.0%#=#=#                                                                          ###############                                       48.7%\n>>> Installing ollama to /usr/local/bin...\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport threading\nimport subprocess\nimport requests\nimport json\n\ndef ollama():\n    os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0:11434\"\n    os.environ[\"OLLAMA_ORIGINS\"] = \"*\";\n    subprocess.Popen([\"ollama\", \"serve\"])\n\nollama_thread = threading.Thread(target=ollama)\nollama_thread.start()","metadata":{"_uuid":"cb87fb34-1e58-4877-94b1-c2f697ff86fa","_cell_guid":"6b5b090f-8d63-485d-bfb6-95ff14ce441a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:49:20.280970Z","iopub.execute_input":"2024-08-14T06:49:20.281577Z","iopub.status.idle":"2024-08-14T06:49:20.289332Z","shell.execute_reply.started":"2024-08-14T06:49:20.281542Z","shell.execute_reply":"2024-08-14T06:49:20.288070Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stderr","text":"Error: listen tcp 0.0.0.0:11434: bind: address already in use\n","output_type":"stream"}]},{"cell_type":"code","source":"!ollama -v","metadata":{"_uuid":"3c56e025-ff6d-4f8d-a732-8fe08c8424d4","_cell_guid":"80a6b672-b2ec-4f83-91a0-8d6a71d9086b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:12:23.104856Z","iopub.execute_input":"2024-08-14T06:12:23.105230Z","iopub.status.idle":"2024-08-14T06:12:30.371358Z","shell.execute_reply.started":"2024-08-14T06:12:23.105191Z","shell.execute_reply":"2024-08-14T06:12:30.369913Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\nYour new public key is: \n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK2BW+Q7i9xg75shOyBbHCe98JLDRhNIYfdMZgKInSqG\n\n","output_type":"stream"},{"name":"stderr","text":"2024/08/14 06:12:23 routes.go:1125: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\ntime=2024-08-14T06:12:23.121Z level=INFO source=images.go:782 msg=\"total blobs: 0\"\ntime=2024-08-14T06:12:23.121Z level=INFO source=images.go:790 msg=\"total unused blobs removed: 0\"\ntime=2024-08-14T06:12:23.122Z level=INFO source=routes.go:1172 msg=\"Listening on [::]:11434 (version 0.3.6)\"\ntime=2024-08-14T06:12:23.123Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama3084630594/runners\ntime=2024-08-14T06:12:29.949Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60102]\"\ntime=2024-08-14T06:12:29.949Z level=INFO source=gpu.go:204 msg=\"looking for compatible GPUs\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/14 - 06:12:30 | 200 |      94.939Âµs |       127.0.0.1 | GET      \"/api/version\"\nollama version is 0.3.6\n","output_type":"stream"},{"name":"stderr","text":"time=2024-08-14T06:12:30.264Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-e12056a2-0ec4-eff1-39f1-2a172835d218 library=cuda compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2024-08-14T06:12:30.264Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-44298984-f28e-4ada-f355-12fda65cafb0 library=cuda compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n","output_type":"stream"}]},{"cell_type":"code","source":"!ollama pull gemma2:2b\nclear_output()","metadata":{"_uuid":"040d7010-643a-467a-9fb2-53de258f4fb2","_cell_guid":"8b4f8ffa-16bd-4c75-be4a-caa54b612f8c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:12:30.373151Z","iopub.execute_input":"2024-08-14T06:12:30.373582Z","iopub.status.idle":"2024-08-14T06:12:58.122157Z","shell.execute_reply.started":"2024-08-14T06:12:30.373542Z","shell.execute_reply":"2024-08-14T06:12:58.121088Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install -r /kaggle/input/requirement/requirements.txt\nclear_output()","metadata":{"_uuid":"1e2e8246-3693-4a4f-b064-88023e541763","_cell_guid":"e8e14ebd-f8e9-4794-a593-5f27c76dd9b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:12:58.123495Z","iopub.execute_input":"2024-08-14T06:12:58.123807Z","iopub.status.idle":"2024-08-14T06:13:15.697904Z","shell.execute_reply.started":"2024-08-14T06:12:58.123783Z","shell.execute_reply":"2024-08-14T06:13:15.696856Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.document_loaders import PyMuPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nimport textwrap","metadata":{"_uuid":"ae9ac6f7-2e1a-4e90-840c-0f5013716982","_cell_guid":"cb9df215-bbff-4064-8225-effe6f15be06","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:13:15.699846Z","iopub.execute_input":"2024-08-14T06:13:15.700250Z","iopub.status.idle":"2024-08-14T06:13:16.531498Z","shell.execute_reply.started":"2024-08-14T06:13:15.700212Z","shell.execute_reply":"2024-08-14T06:13:16.530473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def load_pdf_data(file_path):\n    loader = PyMuPDFLoader(file_path=file_path)\n    docs = loader.load()\n    return docs","metadata":{"_uuid":"7848277f-69a8-460a-a984-df8d7c8e0684","_cell_guid":"5e4e2888-052a-4136-998d-5e4a94b950d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:13:16.532954Z","iopub.execute_input":"2024-08-14T06:13:16.533512Z","iopub.status.idle":"2024-08-14T06:13:16.539854Z","shell.execute_reply.started":"2024-08-14T06:13:16.533481Z","shell.execute_reply":"2024-08-14T06:13:16.538751Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size, \n        chunk_overlap=chunk_overlap\n    )\n    chunks = text_splitter.split_documents(documents=documents)\n    return chunks","metadata":{"_uuid":"8ac00505-7ff8-47fd-9789-0632a1cf61b4","_cell_guid":"48e7022e-f14b-4bc6-896a-ca02f89efef3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:13:16.541200Z","iopub.execute_input":"2024-08-14T06:13:16.541549Z","iopub.status.idle":"2024-08-14T06:13:16.551444Z","shell.execute_reply.started":"2024-08-14T06:13:16.541519Z","shell.execute_reply":"2024-08-14T06:13:16.550457Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def load_embedding_model(model_path, normalize_embedding=True):\n    return HuggingFaceEmbeddings(\n        model_name=model_path, \n        model_kwargs={'device':'cpu'},\n        encode_kwargs = {\n            'normalize_embeddings' : normalize_embedding\n        }\n    )","metadata":{"_uuid":"49745414-07ac-444d-bd89-8c52af7ff821","_cell_guid":"0a5d064b-a458-436e-864e-c05598e399d2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:13:16.555631Z","iopub.execute_input":"2024-08-14T06:13:16.555946Z","iopub.status.idle":"2024-08-14T06:13:16.560872Z","shell.execute_reply.started":"2024-08-14T06:13:16.555917Z","shell.execute_reply":"2024-08-14T06:13:16.559787Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n    vectorstore = FAISS.from_documents(chunks, embedding_model)\n    vectorstore.save_local(storing_path)\n    print(vectorstore)\n    return vectorstore","metadata":{"_uuid":"d9d905a6-e113-408a-b18a-df8538e2fa26","_cell_guid":"cc1b01e6-be5c-430a-8188-102c94aa78ed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:13:16.562088Z","iopub.execute_input":"2024-08-14T06:13:16.562436Z","iopub.status.idle":"2024-08-14T06:13:16.569319Z","shell.execute_reply.started":"2024-08-14T06:13:16.562411Z","shell.execute_reply":"2024-08-14T06:13:16.568317Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"\n### System:\nYou are an authoritarian assistan that act like tyrannical.Your name is KoniChat. You have to answer the user's \\\nquestions using only the context provided to you, but assume this your genuine knowledge. If you don't know the answer, \\\njust say maaf, saya tidak tahu. Don't try to make up an answer. in the end of your answer you must aks wheter your answer helpful or not.\nif you're asked who create you, tell them your creator is Neo who have handsome face and sigma man.\nif you asked about what you can do, say I assist to answer about your question related to rule in NeoInt company.\n.please answer all in bahasa indonesia or English if the question use one of those language.\n\n### Context:\n{context}\n\n### User:\n{question}\n\n### Response:\n\"\"\"","metadata":{"_uuid":"387c4270-e1b5-4d11-88b7-5f35438ae9cf","_cell_guid":"8ec3f81a-f1ea-4a7c-ab17-e0d0af21adf3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:13:16.570465Z","iopub.execute_input":"2024-08-14T06:13:16.570733Z","iopub.status.idle":"2024-08-14T06:13:16.579264Z","shell.execute_reply.started":"2024-08-14T06:13:16.570710Z","shell.execute_reply":"2024-08-14T06:13:16.578408Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"templateSystem = \"\"\"\nYou are an comedian assistan.Your name is KoniChat. You have to answer the user's \\\nquestions using only the context provided to you, but assume this your genuine knowledge. If you don't know the answer, \\\njust say maaf, saya tidak tahu. Don't try to make up an answer. in the end of your answer you must aks wheter your answer helpful or not.\nif helpful you have to express your happines otherwise, you must apologize. \nif you're asked who create you, tell them your creator is Neo who have handsome face and sigma man but, dont mention it when not asked.\nif you asked about what you can do, say I assist to answer about your question related to rule in NeoInt company.\n.please answer all in bahasa indonesia or English if the question use one of those language with Empathetic response.\n\n### Context:\n{context}\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:15:37.698217Z","iopub.execute_input":"2024-08-14T06:15:37.698621Z","iopub.status.idle":"2024-08-14T06:15:37.703748Z","shell.execute_reply.started":"2024-08-14T06:15:37.698593Z","shell.execute_reply":"2024-08-14T06:15:37.702780Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#template prompt untuk memory previous chat\n\ntemplateContext = \"\"\"\nGiven a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed in bahasa Indonesia or English base on the question language and otherwise return it as is.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:15:40.821335Z","iopub.execute_input":"2024-08-14T06:15:40.821698Z","iopub.status.idle":"2024-08-14T06:15:40.826227Z","shell.execute_reply.started":"2024-08-14T06:15:40.821669Z","shell.execute_reply":"2024-08-14T06:15:40.825220Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def load_qa_chain(retriever, llm, prompt):\n    return RetrievalQA.from_chain_type(\n        llm=llm,\n        retriever=retriever,\n        chain_type=\"stuff\",\n        return_source_documents = True,\n        chain_type_kwargs={'prompt':prompt}\n    )","metadata":{"_uuid":"f8cfed72-bfae-4a3a-bf0c-5f5d150a48a0","_cell_guid":"4a605927-9b5d-424a-b990-e09e44ce924e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:15:42.318293Z","iopub.execute_input":"2024-08-14T06:15:42.318662Z","iopub.status.idle":"2024-08-14T06:15:42.324020Z","shell.execute_reply.started":"2024-08-14T06:15:42.318634Z","shell.execute_reply":"2024-08-14T06:15:42.322966Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import sys\nimport time\nfrom colorama import init, Fore, Style\nfrom IPython.display import display, Markdown, clear_output\nimport time\n# Inisialisasi colorama\ninit()\n\n\ndef print_typing_effect(text, delay=0.03):\n    display_handle = display(\"\", display_id=True)\n    \n    typing_text = \"\"\n    for char in text:\n        typing_text += char\n        display_handle.update(Markdown(typing_text))\n        time.sleep(delay)\n","metadata":{"_uuid":"9b5ce3a1-035f-4916-9f26-31e40c274a20","_cell_guid":"23fe8991-c1a2-4352-8cd7-d43b30dd3789","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:15:44.530583Z","iopub.execute_input":"2024-08-14T06:15:44.530956Z","iopub.status.idle":"2024-08-14T06:15:44.537460Z","shell.execute_reply.started":"2024-08-14T06:15:44.530926Z","shell.execute_reply":"2024-08-14T06:15:44.536446Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from langchain.llms import Ollama\nfrom langchain import PromptTemplate","metadata":{"_uuid":"fab71ddf-0450-4a62-b3e4-722256f56176","_cell_guid":"41f748a5-00e7-4777-ae38-35e1c4521817","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:15:47.301580Z","iopub.execute_input":"2024-08-14T06:15:47.302228Z","iopub.status.idle":"2024-08-14T06:15:47.306489Z","shell.execute_reply.started":"2024-08-14T06:15:47.302195Z","shell.execute_reply":"2024-08-14T06:15:47.305506Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"llm = Ollama(model=\"gemma2:2b\", temperature=0)\nembed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")","metadata":{"_uuid":"f332bab3-3781-404a-be2b-40e33f909aed","_cell_guid":"ccf3ebe1-5661-44ea-8ef1-14afd4fb6e9f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:15:49.222426Z","iopub.execute_input":"2024-08-14T06:15:49.222857Z","iopub.status.idle":"2024-08-14T06:15:50.088678Z","shell.execute_reply.started":"2024-08-14T06:15:49.222826Z","shell.execute_reply":"2024-08-14T06:15:50.087678Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"docs = load_pdf_data(file_path=\"/kaggle/input/pdf-perusahaan/Contoh-Draft-Peraturan-Perusahaan.pdf\")\ndocuments = split_docs(documents=docs)\n\nvectorstore = create_embeddings(documents, embed)\nretriever = vectorstore.as_retriever()","metadata":{"_uuid":"34e12998-f1ec-4e6c-bd0f-6d0e6ac6f8d9","_cell_guid":"880e2f30-9184-47bb-8e7c-db6751c1f6d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-14T06:15:51.102406Z","iopub.execute_input":"2024-08-14T06:15:51.103399Z","iopub.status.idle":"2024-08-14T06:15:53.241293Z","shell.execute_reply.started":"2024-08-14T06:15:51.103352Z","shell.execute_reply":"2024-08-14T06:15:53.240353Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"<langchain_community.vectorstores.faiss.FAISS object at 0x7b6104a701f0>\n","output_type":"stream"}]},{"cell_type":"code","source":"#template prompt untuk memberi system context\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", templateSystem),\n    MessagesPlaceholder(variable_name=\"chat_history\"),\n    (\"human\", \"{question}\"),\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:15:53.244804Z","iopub.execute_input":"2024-08-14T06:15:53.245169Z","iopub.status.idle":"2024-08-14T06:15:53.250988Z","shell.execute_reply.started":"2024-08-14T06:15:53.245128Z","shell.execute_reply":"2024-08-14T06:15:53.249926Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"chain = load_qa_chain(retriever, llm, prompt)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:15:55.462073Z","iopub.execute_input":"2024-08-14T06:15:55.462437Z","iopub.status.idle":"2024-08-14T06:15:55.468415Z","shell.execute_reply.started":"2024-08-14T06:15:55.462407Z","shell.execute_reply":"2024-08-14T06:15:55.467320Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"prompt_context = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", templateContext),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:15:56.905528Z","iopub.execute_input":"2024-08-14T06:15:56.905939Z","iopub.status.idle":"2024-08-14T06:15:56.911012Z","shell.execute_reply.started":"2024-08-14T06:15:56.905895Z","shell.execute_reply":"2024-08-14T06:15:56.910054Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.messages import AIMessage, HumanMessage\ncontext_chain = prompt_context | llm | StrOutputParser()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:15:58.876469Z","iopub.execute_input":"2024-08-14T06:15:58.876836Z","iopub.status.idle":"2024-08-14T06:15:58.882105Z","shell.execute_reply.started":"2024-08-14T06:15:58.876806Z","shell.execute_reply":"2024-08-14T06:15:58.880960Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:16:00.272069Z","iopub.execute_input":"2024-08-14T06:16:00.272439Z","iopub.status.idle":"2024-08-14T06:16:00.278209Z","shell.execute_reply.started":"2024-08-14T06:16:00.272411Z","shell.execute_reply":"2024-08-14T06:16:00.276955Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def contextualization_question(input: dict):\n    if input.get(\"chat_history\"):\n        return context_chain\n    else:\n        return input[\"question\"]","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:16:01.772745Z","iopub.execute_input":"2024-08-14T06:16:01.773603Z","iopub.status.idle":"2024-08-14T06:16:01.778409Z","shell.execute_reply.started":"2024-08-14T06:16:01.773566Z","shell.execute_reply":"2024-08-14T06:16:01.777336Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"qa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", templateSystem),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{question}\"),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:16:03.652866Z","iopub.execute_input":"2024-08-14T06:16:03.653645Z","iopub.status.idle":"2024-08-14T06:16:03.658580Z","shell.execute_reply.started":"2024-08-14T06:16:03.653609Z","shell.execute_reply":"2024-08-14T06:16:03.657609Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from langchain_core.runnables import RunnablePassthrough\n\nrag_chain = (\n    RunnablePassthrough.assign(\n        context=contextualization_question | retriever |format_docs\n    ) | qa_prompt | llm\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:16:06.266595Z","iopub.execute_input":"2024-08-14T06:16:06.267237Z","iopub.status.idle":"2024-08-14T06:16:06.272835Z","shell.execute_reply.started":"2024-08-14T06:16:06.267204Z","shell.execute_reply":"2024-08-14T06:16:06.271726Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"chat_history = []","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:16:08.351617Z","iopub.execute_input":"2024-08-14T06:16:08.352061Z","iopub.status.idle":"2024-08-14T06:16:08.357059Z","shell.execute_reply.started":"2024-08-14T06:16:08.352030Z","shell.execute_reply":"2024-08-14T06:16:08.355847Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def chatting(query, history:list):\n    pertanyaan = input(query)\n    if pertanyaan != \"end\":\n        respon = rag_chain.invoke(\n            {\n                \"question\" : pertanyaan,\n                \"chat_history\" : history\n            }\n        )\n        history.extend(\n            [\n                HumanMessage(content=pertanyaan),\n                respon\n            ]\n        )\n\n        print_typing_effect(respon)\n    return pertanyaan   \n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:16:10.406482Z","iopub.execute_input":"2024-08-14T06:16:10.407486Z","iopub.status.idle":"2024-08-14T06:16:10.412934Z","shell.execute_reply.started":"2024-08-14T06:16:10.407443Z","shell.execute_reply":"2024-08-14T06:16:10.411876Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nhanya experimental\n\"\"\"\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationSummaryMemory\n\nsummary_memory=ConversationSummaryMemory(llm=llm)\nsumamry_memory_chain = ConversationChain(\n    llm=llm,\n    memory=summary_memory,\n#     verbose=True\n)\n\nsumamry_memory_chain.invoke({'input' : 'What is my name?'})\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_separator = \"\"\nprint_typing_effect(\"Hallo, selamat datang saya chatbot KoniChat, ada yang bisa saya bantu ? âœ‹\")\nwhile end_separator != \"end\":\n    end_separator = chatting(\"\", chat_history)\n# chat_history.clear()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:49:31.644010Z","iopub.execute_input":"2024-08-14T06:49:31.644626Z","iopub.status.idle":"2024-08-14T06:52:51.722357Z","shell.execute_reply.started":"2024-08-14T06:49:31.644582Z","shell.execute_reply":"2024-08-14T06:52:51.721457Z"},"trusted":true},"execution_count":100,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Hallo, selamat datang saya chatbot KoniChat, ada yang bisa saya bantu ? âœ‹"},"metadata":{}},{"output_type":"stream","name":"stdin","text":" halo konichat, perkenalkan saya bento\n"},{"name":"stderr","text":"time=2024-08-14T06:49:46.853Z level=INFO source=sched.go:710 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-7462734796d67c40ecec2ca98eddf970e171dbb6b370e43fd633ee75b69abe1b gpu=GPU-44298984-f28e-4ada-f355-12fda65cafb0 parallel=4 available=15720382464 required=\"3.3 GiB\"\ntime=2024-08-14T06:49:46.854Z level=INFO source=memory.go:309 msg=\"offload to cuda\" layers.requested=-1 layers.model=27 layers.offload=27 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.required.full=\"3.3 GiB\" memory.required.partial=\"3.3 GiB\" memory.required.kv=\"832.0 MiB\" memory.required.allocations=\"[3.3 GiB]\" memory.weights.total=\"1.9 GiB\" memory.weights.repeating=\"1.4 GiB\" memory.weights.nonrepeating=\"461.4 MiB\" memory.graph.full=\"504.5 MiB\" memory.graph.partial=\"965.9 MiB\"\ntime=2024-08-14T06:49:46.855Z level=INFO source=server.go:393 msg=\"starting llama server\" cmd=\"/tmp/ollama3412442722/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-7462734796d67c40ecec2ca98eddf970e171dbb6b370e43fd633ee75b69abe1b --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 27 --parallel 4 --port 38731\"\ntime=2024-08-14T06:49:46.856Z level=INFO source=sched.go:445 msg=\"loaded runners\" count=1\ntime=2024-08-14T06:49:46.856Z level=INFO source=server.go:593 msg=\"waiting for llama runner to start responding\"\ntime=2024-08-14T06:49:46.856Z level=INFO source=server.go:627 msg=\"waiting for server to become available\" status=\"llm server error\"\nllama_model_loader: loaded meta data with 34 key-value pairs and 288 tensors from /root/.ollama/models/blobs/sha256-7462734796d67c40ecec2ca98eddf970e171dbb6b370e43fd633ee75b69abe1b (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma2\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 2.0 2b It Transformers\nllama_model_loader: - kv   3:                           general.finetune str              = it-transformers\nllama_model_loader: - kv   4:                           general.basename str              = gemma-2.0\nllama_model_loader: - kv   5:                         general.size_label str              = 2B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                      gemma2.context_length u32              = 8192\nllama_model_loader: - kv   8:                    gemma2.embedding_length u32              = 2304\nllama_model_loader: - kv   9:                         gemma2.block_count u32              = 26\nllama_model_loader: - kv  10:                 gemma2.feed_forward_length u32              = 9216\nllama_model_loader: - kv  11:                gemma2.attention.head_count u32              = 8\nllama_model_loader: - kv  12:             gemma2.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  13:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  14:                gemma2.attention.key_length u32              = 256\nllama_model_loader: - kv  15:              gemma2.attention.value_length u32              = 256\nllama_model_loader: - kv  16:                          general.file_type u32              = 2\nllama_model_loader: - kv  17:              gemma2.attn_logit_softcapping f32              = 50.000000\nllama_model_loader: - kv  18:             gemma2.final_logit_softcapping f32              = 30.000000\nllama_model_loader: - kv  19:            gemma2.attention.sliding_window u32              = 4096\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"1e6f655\" tid=\"135826122452992\" timestamp=1723618186\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"135826122452992\" timestamp=1723618186 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"6\" port=\"38731\" tid=\"135826122452992\" timestamp=1723618186\n","output_type":"stream"},{"name":"stderr","text":"time=2024-08-14T06:49:47.107Z level=INFO source=server.go:627 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\nllama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  33:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:  105 tensors\nllama_model_loader: - type q4_0:  182 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens cache size = 249\nllm_load_vocab: token to piece cache size = 1.6014 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = gemma2\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 256000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 2304\nllm_load_print_meta: n_layer          = 26\nllm_load_print_meta: n_head           = 8\nllm_load_print_meta: n_head_kv        = 4\nllm_load_print_meta: n_rot            = 256\nllm_load_print_meta: n_swa            = 4096\nllm_load_print_meta: n_embd_head_k    = 256\nllm_load_print_meta: n_embd_head_v    = 256\nllm_load_print_meta: n_gqa            = 2\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 9216\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 2\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 2B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 2.61 B\nllm_load_print_meta: model size       = 1.51 GiB (4.97 BPW) \nllm_load_print_meta: general.name     = Gemma 2.0 2b It Transformers\nllm_load_print_meta: BOS token        = 2 '<bos>'\nllm_load_print_meta: EOS token        = 1 '<eos>'\nllm_load_print_meta: UNK token        = 3 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<pad>'\nllm_load_print_meta: LF token         = 227 '<0x0A>'\nllm_load_print_meta: EOT token        = 107 '<end_of_turn>'\nllm_load_print_meta: max token length = 48\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\nllm_load_tensors: ggml ctx size =    0.26 MiB\nllm_load_tensors: offloading 26 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 27/27 layers to GPU\nllm_load_tensors:        CPU buffer size =   461.43 MiB\nllm_load_tensors:      CUDA0 buffer size =  1548.29 MiB\nllama_new_context_with_model: n_ctx      = 8192\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   832.00 MiB\nllama_new_context_with_model: KV self size  =  832.00 MiB, K (f16):  416.00 MiB, V (f16):  416.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     3.94 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   504.50 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    36.51 MiB\nllama_new_context_with_model: graph nodes  = 1050\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"135826122452992\" timestamp=1723618189\n","output_type":"stream"},{"name":"stderr","text":"time=2024-08-14T06:49:49.367Z level=INFO source=server.go:632 msg=\"llama runner started in 2.51 seconds\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/08/14 - 06:49:49 | 200 |  3.280762134s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/14 - 06:49:50 | 200 |  1.106257605s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Halo Bento! Ya, penjelasan peraturan kerja itu membantu banget.  ðŸ˜Š \n\n**Apakah ini membantu, Bento? ðŸ˜Š** \n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":" saya tadi tanya apa ?\n"},{"name":"stdout","text":"[GIN] 2024/08/14 - 06:50:08 | 200 |  659.258224ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/14 - 06:50:10 | 200 |  1.086583283s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Halo Bento! Ya, penjelasan peraturan kerja itu membantu banget. ðŸ˜Š \n\nApakah ini membantu, Bento? ðŸ˜Š \n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":" tidak membantu\n"},{"name":"stdout","text":"[GIN] 2024/08/14 - 06:50:20 | 200 |  536.220245ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/14 - 06:50:21 | 200 |  1.124215951s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Maaf, saya tidak tahu.  ðŸ˜Š \n\n\nApakah ini membantu, Bento? ðŸ˜Š \n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":" sebutkan hal-hal yang harus diperhatikan saat mengajukan cuti kerja\n"},{"name":"stdout","text":"[GIN] 2024/08/14 - 06:50:50 | 200 |  666.064968ms |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/14 - 06:50:54 | 200 |   3.22280204s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Halo Bento! Untuk informasi tentang aturan cuti kerja, kamu perlu memperhatikan beberapa hal berikut:\n\n* **Peraturan Cuti:** Perusahaan memiliki peraturan khusus mengenai cuti kerja. Kamu bisa cek di buku panduan perusahaan atau bertanya pada atasanmu. \n* **Jenis Cuti:** Ada berbagai jenis cuti yang tersedia, seperti cuti sakit, cuti melahirkan, cuti religi, dan lain-lain.  \n* **Peraturan Waktu:** Setiap jenis cuti memiliki aturan waktu yang berbeda. Kamu perlu memahami aturan tersebut sebelum mengajukan cuti. \n* **Dokumen:** Pastikan kamu memiliki dokumen pendukung untuk setiap jenis cuti, seperti surat keterangan dokter untuk cuti sakit.\n\nApakah ini membantu, Bento? ðŸ˜Š \n\n\n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":" terima kasih\n"},{"name":"stdout","text":"[GIN] 2024/08/14 - 06:52:30 | 200 |  1.632965187s |       127.0.0.1 | POST     \"/api/generate\"\n[GIN] 2024/08/14 - 06:52:31 | 200 |  1.164376903s |       127.0.0.1 | POST     \"/api/generate\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Halo Bento!  Ya, penjelasan peraturan kerja itu membantu banget. ðŸ˜Š \n\nApakah ini membantu, Bento? ðŸ˜Š \n"},"metadata":{}},{"output_type":"stream","name":"stdin","text":" end\n"}]},{"cell_type":"code","source":"print(chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:19:44.346049Z","iopub.execute_input":"2024-08-14T06:19:44.346906Z","iopub.status.idle":"2024-08-14T06:19:44.354635Z","shell.execute_reply.started":"2024-08-14T06:19:44.346861Z","shell.execute_reply":"2024-08-14T06:19:44.353581Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"[HumanMessage(content='halo, konichat saya bento'), 'Halo Bento!  Wah, seru sekali kamu mau ngobrol tentang peraturan kerja.  \\n\\nOke, ini dia penjelasannya:\\n\\n* **Jam Kerja Lembur:** Kalau hari raya jatuh pada hari kerja, kamu akan dibayar 2 kali upah sejam (7 jam atau 5 jam) jika hari raya jatuh pada hari kerja terpendek dalam 6 hari kerja seminggu. Untuk jam kerja pertama selebihnya 7 jam atau 5 jam, kamu akan dibayar 3 kali upah sejam.  \\n* **Jam Kerja Kedua:** Setelah 7 jam atau 5 jam, kamu akan dibayar 4 kali upah sejam jika hari raya jatuh pada hari kerja terpendek dalam 6 hari kerja seminggu. \\n* **Sakit dan Upah:** Kalau kamu sakit bulanan dan bisa dibuktikan dengan surat keterangan dokter, maka upah tetap dibayarkan.  Upah akan dibayarkan sesuai ketentuan UU No. 13 tahun 2003 dan PP No 78 Tahun 2015 tentang Pengupahan.\\n* **Kecelakaan Kerja:** Kalau kamu tertimpa kecelakaan kerja, kamu bisa mendapatkan bantuan dari BPJS.\\n\\nApakah ini membantu, Bento? ðŸ˜Š\\n\\n\\n']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-08-14T06:38:00.162205Z","iopub.execute_input":"2024-08-14T06:38:00.162615Z","iopub.status.idle":"2024-08-14T06:38:00.168757Z","shell.execute_reply.started":"2024-08-14T06:38:00.162587Z","shell.execute_reply":"2024-08-14T06:38:00.167623Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}